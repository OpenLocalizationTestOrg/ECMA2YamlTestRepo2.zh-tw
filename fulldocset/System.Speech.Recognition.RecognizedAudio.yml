### YamlMime:ManagedReference
items:
- uid: System.Speech.Recognition.RecognizedAudio
  id: RecognizedAudio
  children:
  - System.Speech.Recognition.RecognizedAudio.AudioPosition
  - System.Speech.Recognition.RecognizedAudio.Duration
  - System.Speech.Recognition.RecognizedAudio.Format
  - System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)
  - System.Speech.Recognition.RecognizedAudio.StartTime
  - System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)
  - System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)
  langs:
  - csharp
  name: RecognizedAudio
  nameWithType: RecognizedAudio
  fullName: System.Speech.Recognition.RecognizedAudio
  type: Class
  summary: "表示音訊輸入也就是相關聯<xref href=&quot;System.Speech.Recognition.RecognitionResult&quot;> </xref>。"
  remarks: "語音辨識器產生的音訊輸入有關辨識作業的資訊。 若要存取的可辨識的音訊，請使用<xref:System.Speech.Recognition.RecognitionResult.Audio%2A>屬性或<xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A>方法的<xref:System.Speech.Recognition.RecognitionResult>。</xref:System.Speech.Recognition.RecognitionResult> </xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> </xref:System.Speech.Recognition.RecognitionResult.Audio%2A>       下列事件和方法可產生辨識結果<xref:System.Speech.Recognition.SpeechRecognizer>和<xref:System.Speech.Recognition.SpeechRecognitionEngine>類別:-事件:-<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName>和<xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName>-<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName>和<xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName>-<xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName>和<xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>-<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName>和<xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName>- <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName>-方法:-<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName>和<xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName>-<xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName>和<xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName>- <xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName>- <xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName>> [!IMPORTANT] > A 辨識模擬的語音辨識所產生的結果不包含可辨識的音訊。</xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeAsync%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.Recognize%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeAsync%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeAsync%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognize%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognize%2A?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.RecognizeCompleted?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.EmulateRecognizeCompleted?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.EmulateRecognizeCompleted?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognitionRejected?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognitionRejected?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognizer.SpeechHypothesized?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechHypothesized?displayProperty=fullName> </xref:System.Speech.Recognition.SpeechRecognitionEngine> </xref:System.Speech.Recognition.SpeechRecognizer> 這類的辨識結果，其<xref:System.Speech.Recognition.RecognitionResult.Audio%2A>屬性會傳回`null`及其<xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A>方法會擲回的例外狀況。</xref:System.Speech.Recognition.RecognitionResult.GetAudioForWordRange%2A> </xref:System.Speech.Recognition.RecognitionResult.Audio%2A> 如需有關模擬的語音辨識的詳細資訊，請參閱`EmulateRecognize`和`EmulateRecognizeAsync`方法<xref:System.Speech.Recognition.SpeechRecognizer>和<xref:System.Speech.Recognition.SpeechRecognitionEngine>類別。</xref:System.Speech.Recognition.SpeechRecognitionEngine> </xref:System.Speech.Recognition.SpeechRecognizer>"
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName>, <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName>, or <xref:System.Speech.Recognition.Grammar.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public class RecognizedAudio
  inheritance:
  - System.Object
  implements: []
  inheritedMembers: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition
  id: AudioPosition
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: AudioPosition
  nameWithType: RecognizedAudio.AudioPosition
  fullName: System.Speech.Recognition.RecognizedAudio.AudioPosition
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "取得輸入的音訊資料流中的位置開始的可辨識音訊。"
  remarks: "這個屬性會參考開頭位置的已辨識片語的輸入的裝置產生的音訊資料流中。 相反地，`RecognizerAudioPosition`屬性<xref:System.Speech.Recognition.SpeechRecognitionEngine>和<xref:System.Speech.Recognition.SpeechRecognizer>類別參考其音訊的輸入中的辨識器的位置。</xref:System.Speech.Recognition.SpeechRecognizer> </xref:System.Speech.Recognition.SpeechRecognitionEngine> 這些位置可能會不同。 如需詳細資訊，請參閱[使用語音辨識事件](http://msdn.microsoft.com/en-us/01c598ca-2e0e-4e89-b303-cd1cef9e8482)。       <xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A>屬性會取得系統時間的辨識作業開頭。</xref:System.Speech.Recognition.RecognizedAudio.StartTime%2A>"
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public TimeSpan AudioPosition { get; }
    return:
      type: System.TimeSpan
      description: "可辨識音訊中開始輸入音訊資料流的位置。"
  overload: System.Speech.Recognition.RecognizedAudio.AudioPosition*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.Duration
  id: Duration
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: Duration
  nameWithType: RecognizedAudio.Duration
  fullName: System.Speech.Recognition.RecognizedAudio.Duration
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "取得可辨識音訊輸入音訊資料流的持續時間。"
  remarks: ''
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public TimeSpan Duration { get; }
    return:
      type: System.TimeSpan
      description: "持續時間輸入音訊資料流中的可辨識音訊。"
  overload: System.Speech.Recognition.RecognizedAudio.Duration*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.Format
  id: Format
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: Format
  nameWithType: RecognizedAudio.Format
  fullName: System.Speech.Recognition.RecognizedAudio.Format
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "取得辨識引擎處理的音訊格式。"
  remarks: ''
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public System.Speech.AudioFormat.SpeechAudioFormatInfo Format { get; }
    return:
      type: System.Speech.AudioFormat.SpeechAudioFormatInfo
      description: "語音辨識器所處理的音訊格式。"
  overload: System.Speech.Recognition.RecognizedAudio.Format*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)
  id: GetRange(System.TimeSpan,System.TimeSpan)
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: GetRange(TimeSpan,TimeSpan)
  nameWithType: RecognizedAudio.GetRange(TimeSpan,TimeSpan)
  fullName: System.Speech.Recognition.RecognizedAudio.GetRange(TimeSpan,TimeSpan)
  type: Method
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "選取並傳回一段目前辨識音訊為二進位資料。"
  remarks: ''
  example:
  - "The following example creates a speech recognition grammar for name input, adds a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event, and loads the grammar into an in-process speech recognizer. Then it writes the audio information for the name portion of the input to an audio file. The audio file is used as input to a <xref:System.Speech.Synthesis.SpeechSynthesizer> object, which speaks a phrase that includes the recorded audio.  \n  \n```  \nprivate static void AddNameGrammar(SpeechRecognitionEngine recognizer)  \n{  \n  GrammarBuilder builder = new GrammarBuilder();  \n  builder.Append(\"My name is\");  \n  builder.AppendWildcard();  \n  \n  Grammar nameGrammar = new Grammar(builder);  \n  nameGrammar.Name = \"Name Grammar\";  \n  nameGrammar.SpeechRecognized +=  \n    new EventHandler<SpeechRecognizedEventArgs>(  \n      NameSpeechRecognized);  \n  \n  recognizer.LoadGrammar(nameGrammar);  \n}  \n  \n// Handle the SpeechRecognized event of the name grammar.  \nprivate static void NameSpeechRecognized(  \n  object sender, SpeechRecognizedEventArgs e)  \n{  \n  Console.WriteLine(\"Grammar ({0}) recognized speech: {1}\",  \n    e.Result.Grammar.Name, e.Result.Text);  \n  \n  try  \n  {  \n  \n    // The name phrase starts after the first three words.  \n    if (e.Result.Words.Count < 4)  \n    {  \n  \n      // Add code to check for an alternate that contains the wildcard.  \n      return;  \n    }  \n  \n    RecognizedAudio audio = e.Result.Audio;  \n    TimeSpan start = e.Result.Words[3].AudioPosition;  \n    TimeSpan duration = audio.Duration - start;  \n  \n    // Add code to verify and persist the audio.  \n    string path = @\"C:\\temp\\nameAudio.wav\";  \n    using (Stream outputStream = new FileStream(path, FileMode.Create))  \n    {  \n      RecognizedAudio nameAudio = audio.GetRange(start, duration);  \n      nameAudio.WriteToWaveStream(outputStream);  \n      outputStream.Close();  \n    }  \n  \n    Thread testThread =  \n      new Thread(new ParameterizedThreadStart(TestAudio));  \n    testThread.Start(path);  \n  }  \n  catch (Exception ex)  \n  {  \n    Console.WriteLine(\"Exception thrown while processing audio:\");  \n    Console.WriteLine(ex.ToString());  \n  }  \n}  \n  \n// Use the speech synthesizer to play back the .wav file  \n// that was created in the SpeechRecognized event handler.  \n  \nprivate static void TestAudio(object item)  \n{  \n  string path = item as string;  \n  if (path != null && File.Exists(path))  \n  {  \n    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  \n    PromptBuilder builder = new PromptBuilder();  \n    builder.AppendText(\"Hello\");  \n    builder.AppendAudio(path);  \n    synthesizer.Speak(builder);  \n  }  \n}  \n```"
  syntax:
    content: public System.Speech.Recognition.RecognizedAudio GetRange (TimeSpan audioPosition, TimeSpan duration);
    parameters:
    - id: audioPosition
      type: System.TimeSpan
      description: "音訊資料要傳回的起點。"
    - id: duration
      type: System.TimeSpan
      description: "要傳回的區段長度。"
    return:
      type: System.Speech.Recognition.RecognizedAudio
      description: "依照所定義，傳回的子區段的可辨識音訊<code> audioPosition </code>和<code> duration </code>。"
  overload: System.Speech.Recognition.RecognizedAudio.GetRange*
  exceptions:
  - type: System.ArgumentOutOfRangeException
    commentId: T:System.ArgumentOutOfRangeException
    description: "<code>audioPosition</code>和<code>duration</code>定義目前區段的範圍之外的音訊的區段。"
  - type: System.InvalidOperationException
    commentId: T:System.InvalidOperationException
    description: "目前辨識音訊未包含任何資料。"
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.StartTime
  id: StartTime
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: StartTime
  nameWithType: RecognizedAudio.StartTime
  fullName: System.Speech.Recognition.RecognizedAudio.StartTime
  type: Property
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "取得系統時間的辨識作業開頭。"
  remarks: "StartTime 屬性會取得系統時間辨識作業，這會很有用的延遲和效能計算的開頭。       <xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A>屬性會取得輸入的裝置產生的音訊資料流中的位置。</xref:System.Speech.Recognition.RecognizedAudio.AudioPosition%2A>"
  example:
  - "The following example handles the <xref:System.Speech.Recognition.SpeechRecognitionEngine.SpeechRecognized?displayProperty=fullName> or <xref:System.Speech.Recognition.SpeechRecognizer.SpeechRecognized?displayProperty=fullName> event and outputs to the console information about the recognized audio that is associated with the recognition result.  \n  \n```c#  \n  \n// Handle the SpeechRecognized event.   \nvoid SpeechRecognizedHandler(object sender, SpeechRecognizedEventArgs e)  \n{  \n  if (e.Result == null) return;  \n  \n  RecognitionResult result = e.Result;  \n  \n  Console.WriteLine(\"Grammar({0}): {1}\",  \n    result.Grammar.Name, result.Text);  \n  \n  if (e.Result.Audio != null)  \n  {  \n    RecognizedAudio audio = e.Result.Audio;  \n  \n    Console.WriteLine(\"   start time: {0}\", audio.StartTime);  \n    Console.WriteLine(\"   encoding format: {0}\", audio.Format.EncodingFormat);  \n    Console.WriteLine(\"   position: {0}, duration: {1}\",  \n      audio.AudioPosition, audio.Duration);  \n  }  \n  \n  // Add event handler code here.  \n}  \n```"
  syntax:
    content: public DateTime StartTime { get; }
    return:
      type: System.DateTime
      description: "系統在辨識作業開始時間。"
  overload: System.Speech.Recognition.RecognizedAudio.StartTime*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)
  id: WriteToAudioStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: WriteToAudioStream(Stream)
  nameWithType: RecognizedAudio.WriteToAudioStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(Stream)
  type: Method
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "做為原始資料寫入資料流的整個音訊。"
  remarks: "音訊資料寫入至`outputStream`二進位格式。 標頭不未包含任何資訊。       WriteToAudioStream 方法使用 Wave 格式，但不包含 Wave 標頭。 若要包含 Wave 標頭，使用<xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A>方法。</xref:System.Speech.Recognition.RecognizedAudio.WriteToWaveStream%2A>"
  syntax:
    content: public void WriteToAudioStream (System.IO.Stream outputStream);
    parameters:
    - id: outputStream
      type: System.IO.Stream
      description: "將接收音訊資料流。"
  overload: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream*
  exceptions: []
  platform:
  - net462
- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)
  id: WriteToWaveStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  langs:
  - csharp
  name: WriteToWaveStream(Stream)
  nameWithType: RecognizedAudio.WriteToWaveStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(Stream)
  type: Method
  assemblies:
  - System.Speech
  namespace: System.Speech.Recognition
  summary: "Wave 格式的資料流中寫入音訊。"
  remarks: "音訊資料寫入至`outputStream`Wave 格式，包括資源交換檔案格式 (RIFF) 標頭。       <xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A>方法會使用相同的二進位格式，但不包含 Wave 標頭。</xref:System.Speech.Recognition.RecognizedAudio.WriteToAudioStream%2A>"
  example:
  - "The following example creates a speech recognition grammar for name input, adds a handler for the <xref:System.Speech.Recognition.Grammar.SpeechRecognized> event, and loads the grammar into an in-process speech recognizer. Then it writes the audio information for the name portion of the input to an audio file. The audio file is used as input to a <xref:System.Speech.Synthesis.SpeechSynthesizer> object, which speaks a phrase that includes the recorded audio.  \n  \n```  \nprivate static void AddNameGrammar(SpeechRecognitionEngine recognizer)  \n{  \n  GrammarBuilder builder = new GrammarBuilder();  \n  builder.Append(\"My name is\");  \n  builder.AppendWildcard();  \n  \n  Grammar nameGrammar = new Grammar(builder);  \n  nameGrammar.Name = \"Name Grammar\";  \n  nameGrammar.SpeechRecognized +=  \n    new EventHandler<SpeechRecognizedEventArgs>(  \n      NameSpeechRecognized);  \n  \n  recognizer.LoadGrammar(nameGrammar);  \n}  \n  \n// Handle the SpeechRecognized event of the name grammar.  \nprivate static void NameSpeechRecognized(  \n  object sender, SpeechRecognizedEventArgs e)  \n{  \n  Console.WriteLine(\"Grammar ({0}) recognized speech: {1}\",  \n    e.Result.Grammar.Name, e.Result.Text);  \n  \n  try  \n  {  \n    // The name phrase starts after the first three words.  \n    if (e.Result.Words.Count < 4)  \n    {  \n  \n      // Add code to check for an alternate that contains the   \nwildcard.  \n      return;  \n    }  \n  \n    RecognizedAudio audio = e.Result.Audio;  \n    TimeSpan start = e.Result.Words[3].AudioPosition;  \n    TimeSpan duration = audio.Duration - start;  \n  \n    // Add code to verify and persist the audio.  \n    string path = @\"C:\\temp\\nameAudio.wav\";  \n    using (Stream outputStream = new FileStream(path, FileMode.Create))  \n    {  \n      RecognizedAudio nameAudio = audio.GetRange(start, duration);  \n      nameAudio.WriteToWaveStream(outputStream);  \n      outputStream.Close();  \n    }  \n  \n    Thread testThread =  \n      new Thread(new ParameterizedThreadStart(TestAudio));  \n    testThread.Start(path);  \n  }  \n  catch (Exception ex)  \n  {  \n    Console.WriteLine(\"Exception thrown while processing audio:\");  \n    Console.WriteLine(ex.ToString());  \n  }  \n}  \n  \n// Use the speech synthesizer to play back the .wav file  \n// that was created in the SpeechRecognized event handler.  \n  \nprivate static void TestAudio(object item)  \n{  \n  string path = item as string;  \n  if (path != null && File.Exists(path))  \n  {  \n    SpeechSynthesizer synthesizer = new SpeechSynthesizer();  \n    PromptBuilder builder = new PromptBuilder();  \n    builder.AppendText(\"Hello\");  \n    builder.AppendAudio(path);  \n    synthesizer.Speak(builder);  \n  }  \n}  \n```"
  syntax:
    content: public void WriteToWaveStream (System.IO.Stream outputStream);
    parameters:
    - id: outputStream
      type: System.IO.Stream
      description: "將接收音訊資料流。"
  overload: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream*
  exceptions: []
  platform:
  - net462
references:
- uid: System.Object
  isExternal: false
  name: System.Object
- uid: System.ArgumentOutOfRangeException
  isExternal: true
  name: System.ArgumentOutOfRangeException
- uid: System.InvalidOperationException
  isExternal: true
  name: System.InvalidOperationException
- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: AudioPosition
  nameWithType: RecognizedAudio.AudioPosition
  fullName: System.Speech.Recognition.RecognizedAudio.AudioPosition
- uid: System.TimeSpan
  parent: System
  isExternal: true
  name: TimeSpan
  nameWithType: TimeSpan
  fullName: System.TimeSpan
- uid: System.Speech.Recognition.RecognizedAudio.Duration
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Duration
  nameWithType: RecognizedAudio.Duration
  fullName: System.Speech.Recognition.RecognizedAudio.Duration
- uid: System.Speech.Recognition.RecognizedAudio.Format
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Format
  nameWithType: RecognizedAudio.Format
  fullName: System.Speech.Recognition.RecognizedAudio.Format
- uid: System.Speech.AudioFormat.SpeechAudioFormatInfo
  parent: System.Speech.AudioFormat
  isExternal: false
  name: SpeechAudioFormatInfo
  nameWithType: SpeechAudioFormatInfo
  fullName: System.Speech.AudioFormat.SpeechAudioFormatInfo
- uid: System.Speech.Recognition.RecognizedAudio.GetRange(System.TimeSpan,System.TimeSpan)
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: GetRange(TimeSpan,TimeSpan)
  nameWithType: RecognizedAudio.GetRange(TimeSpan,TimeSpan)
  fullName: System.Speech.Recognition.RecognizedAudio.GetRange(TimeSpan,TimeSpan)
- uid: System.Speech.Recognition.RecognizedAudio
  parent: System.Speech.Recognition
  isExternal: false
  name: RecognizedAudio
  nameWithType: RecognizedAudio
  fullName: System.Speech.Recognition.RecognizedAudio
- uid: System.Speech.Recognition.RecognizedAudio.StartTime
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: StartTime
  nameWithType: RecognizedAudio.StartTime
  fullName: System.Speech.Recognition.RecognizedAudio.StartTime
- uid: System.DateTime
  parent: System
  isExternal: true
  name: DateTime
  nameWithType: DateTime
  fullName: System.DateTime
- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToAudioStream(Stream)
  nameWithType: RecognizedAudio.WriteToAudioStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream(Stream)
- uid: System.IO.Stream
  parent: System.IO
  isExternal: true
  name: Stream
  nameWithType: Stream
  fullName: System.IO.Stream
- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(System.IO.Stream)
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToWaveStream(Stream)
  nameWithType: RecognizedAudio.WriteToWaveStream(Stream)
  fullName: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream(Stream)
- uid: System.Speech.Recognition.RecognizedAudio.AudioPosition*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: AudioPosition
  nameWithType: RecognizedAudio.AudioPosition
- uid: System.Speech.Recognition.RecognizedAudio.Duration*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Duration
  nameWithType: RecognizedAudio.Duration
- uid: System.Speech.Recognition.RecognizedAudio.Format*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: Format
  nameWithType: RecognizedAudio.Format
- uid: System.Speech.Recognition.RecognizedAudio.GetRange*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: GetRange
  nameWithType: RecognizedAudio.GetRange
- uid: System.Speech.Recognition.RecognizedAudio.StartTime*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: StartTime
  nameWithType: RecognizedAudio.StartTime
- uid: System.Speech.Recognition.RecognizedAudio.WriteToAudioStream*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToAudioStream
  nameWithType: RecognizedAudio.WriteToAudioStream
- uid: System.Speech.Recognition.RecognizedAudio.WriteToWaveStream*
  parent: System.Speech.Recognition.RecognizedAudio
  isExternal: false
  name: WriteToWaveStream
  nameWithType: RecognizedAudio.WriteToWaveStream
